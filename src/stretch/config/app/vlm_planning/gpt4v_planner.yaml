# Xinwei changed
vlm_base_config: app/vlm_planning/gpt4v_planner.yaml
use_scene_graph: True
add_local_every_step: false
min_points_per_voxel: 15
exp_min_density: 1

encoder: "dinov2siglip"
encoder_args: 
  model_name: "google/siglip-base-patch16-224"
tts_engine: "gTTS"
open_vocab_category_map_file: example_cat_map.json

task:
  command: "move the yellow box to the tv area"  # Example: "navigate_to_kitchen"

# Sparse Voxel Map parameters
voxel_size: 0.01
obs_min_height: 0.1  # Ignore things less than this high
obs_max_height: 1.8  # Ignore things over this height (eg ceilings)
obs_min_density: 5  # This many points makes it an obstacle
smooth_kernel_size: 0

# Padding
# pad_obstacles: 2  # Add this many units (voxel_size) to the area around obstacles
pad_obstacles: 1  # Add this many units (voxel_size) to the area around obstacles
min_pad_obstacles: 0  # Do not pad LESS than this amount, for safety.

local_radius: 0.8  # Area around the robot to mark as explored (kind of a hack)
remove_visited_from_obstacles: False
min_depth: 0.5
# max_depth: 2.5
max_depth: 2.0

instance_memory:
  min_instance_thickness: 0.01 # filter out instances that are too flat like floors, walls etc
  min_instance_vol: 1e-4 # filter out instances that are of too small volume
  max_instance_vol: 0.5 # filter out instances that are too large
  min_instance_height: 0.4
  max_instance_height: 1.5
  min_pixels_for_instance_view: 100
  min_percent_for_instance_view: 0.1
  use_visual_feat: True

  matching:
    # Feature matching threshold for if something is considered a particular class
    # Set this value by experimting with:
    #   python -m stretch.app.query --threshold 0.05
    # You can change the threshold to anything that makes sense.
    feature_match_threshold: 0.05

# Object detection parameters
detection:
  module: detic
  category_map_file: example_cat_map.json

# Point cloud cleanup
filters:
  smooth_kernel_size: 2
  #smooth_kernel_size: 0
  use_median_filter: True
  median_filter_size: 2
  median_filter_max_error: 0.01
  use_derivative_filter: False
  derivative_filter_threshold: 0.1
  # use_voxel_filter: True

# Motion convergence parameters
# These are to make sure the robot is not doing anything weird
motion:
  moving_threshold: 0.001  # How much the robot has to move to be considered "moving"
  angle_threshold: 0.01  # How much the robot has to rotate to be considered "rotating"
  min_steps_not_moving: 2  # How many steps the robot has to not move before we consider it "stopped"
  joint_tolerance:
    arm: 0.05
    base_x: 0.05
    lift: 0.05
    wrist_roll: 0.25
    wrist_pitch: 0.25
    wrist_yaw: 0.05
    head_pan: 0.01
    head_tilt: 0.01
  joint_thresholds:
    head_not_moving_tolerance: 1.0e-4


# Exploration
in_place_rotation_steps: 8

# TAMP parameters
guarantee_instance_is_reachable: True
plan_with_reachable_instances: True
plan_with_scene_graph: True
scene_graph:
  max_near_distance: 2
  min_on_height: 0.05
  max_on_height: 0.2

# Motion planner parameters
motion_planner:
  # Navigation space - used for motion planning and computing goals.
  step_size: 0.1
  rotation_step_size: 0.2 

  simplify_plans: True
  shortcut_plans: True
  shortcut_iter: 100
  # Parameters for frontier exploration using the motion planner.
  frontier:
    dilate_frontier_size: 3  # Used to shrink the frontier back from the edges of the world
    dilate_obstacle_size: 4  # Used when selecting goals and computing what the "frontier" is

    default_expand_frontier_size: 10  # margin along the frontier where final robot position can be
    # Distance away you search for frontier points
    min_dist: 0.1
    # Subsampling frontier space at this discretization
    step_dist: 0.1
  simplify:
    min_step: 0.1
    max_step: 0.5
    num_steps: 10
    min_angle: 0.1
  goals:
    manipulation_radius: 0.55

# Trajectory following - how closely we follow intermediate waypoints
# These should be less strict than whatever parameters the low-level controller is using; this will
# make sure that the motions end up looking smooth.
trajectory_pos_err_threshold: 0.15
trajectory_rot_err_threshold: 0.3
trajectory_per_step_timeout: 3.0

# User interface
# Choose one of: (object_to_find, location_to_place), command, or chat
# Don't use all of them!
name: "stretch"  # for logging - currently not used
vlm_context_length: 20  # How long messages sent to the vlm server can be if we are using it
limited_obs_publish_sleep: 0.5

# High level stuff: commands to execute 
# command: "pick up a bottle and put it on the chair"
# name: "spot"
exploration_steps: 10 # we can potentially increase it to 150 for sim
# object_to_find: "bottle"
# location_to_place: "chair"

# VLM Query parameters
sample_strategy: "clip"
vlm_option: gpt4v
replanning: False # if replanning is True, the robot agent will memorize the task that is given at the first place
save_vlm_plan: True